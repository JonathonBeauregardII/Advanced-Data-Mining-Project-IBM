{
    "nbformat": 4, 
    "cells": [
        {
            "cell_type": "code", 
            "execution_count": 1, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "import pandas as pd\nfrom pyspark.sql import SparkSession\n", 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "execution_count": 2, 
            "metadata": {
                "collapsed": false
            }, 
            "source": "# The code was removed by DSX for sharing.", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "[Row(_c0=1, CASE_STATUS=u'CERTIFIED-WITHDRAWN', EMPLOYER_NAME=u'UNIVERSITY OF MICHIGAN', SOC_NAME=u'BIOCHEMISTS AND BIOPHYSICISTS', JOB_TITLE=u'POSTDOCTORAL RESEARCH FELLOW', FULL_TIME_POSITION=u'N', PREVAILING_WAGE=u'36067', YEAR=u'2016', WORKSITE=u'ANN ARBOR, MICHIGAN', lon=u'-83.7430378', lat=u'42.2808256'),\n Row(_c0=2, CASE_STATUS=u'CERTIFIED-WITHDRAWN', EMPLOYER_NAME=u'GOODMAN NETWORKS, INC.', SOC_NAME=u'CHIEF EXECUTIVES', JOB_TITLE=u'CHIEF OPERATING OFFICER', FULL_TIME_POSITION=u'Y', PREVAILING_WAGE=u'242674', YEAR=u'2016', WORKSITE=u'PLANO, TEXAS', lon=u'-96.6988856', lat=u'33.0198431'),\n Row(_c0=3, CASE_STATUS=u'CERTIFIED-WITHDRAWN', EMPLOYER_NAME=u'PORTS AMERICA GROUP, INC.', SOC_NAME=u'CHIEF EXECUTIVES', JOB_TITLE=u'CHIEF PROCESS OFFICER', FULL_TIME_POSITION=u'Y', PREVAILING_WAGE=u'193066', YEAR=u'2016', WORKSITE=u'JERSEY CITY, NEW JERSEY', lon=u'-74.0776417', lat=u'40.7281575'),\n Row(_c0=4, CASE_STATUS=u'CERTIFIED-WITHDRAWN', EMPLOYER_NAME=u'GATES CORPORATION, A WHOLLY-OWNED SUBSIDIARY OF TOMKINS PLC', SOC_NAME=u'CHIEF EXECUTIVES', JOB_TITLE=u'REGIONAL PRESIDEN, AMERICAS', FULL_TIME_POSITION=u'Y', PREVAILING_WAGE=u'220314', YEAR=u'2016', WORKSITE=u'DENVER, COLORADO', lon=u'-104.990251', lat=u'39.7392358'),\n Row(_c0=5, CASE_STATUS=u'WITHDRAWN', EMPLOYER_NAME=u'PEABODY INVESTMENTS CORP.', SOC_NAME=u'CHIEF EXECUTIVES', JOB_TITLE=u'PRESIDENT MONGOLIA AND INDIA', FULL_TIME_POSITION=u'Y', PREVAILING_WAGE=u'157518.4', YEAR=u'2016', WORKSITE=u'ST. LOUIS, MISSOURI', lon=u'-90.1994042', lat=u'38.6270025')]"
                    }, 
                    "execution_count": 2, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": 3, 
            "metadata": {
                "collapsed": false
            }, 
            "source": "df_data_1.describe()", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "DataFrame[summary: string, _c0: string]"
                    }, 
                    "execution_count": 3, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": 4, 
            "metadata": {
                "collapsed": false
            }, 
            "source": "df_data_1.printSchema()", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "root\n |-- _c0: integer (nullable = true)\n |-- CASE_STATUS: string (nullable = true)\n |-- EMPLOYER_NAME: string (nullable = true)\n |-- SOC_NAME: string (nullable = true)\n |-- JOB_TITLE: string (nullable = true)\n |-- FULL_TIME_POSITION: string (nullable = true)\n |-- PREVAILING_WAGE: string (nullable = true)\n |-- YEAR: string (nullable = true)\n |-- WORKSITE: string (nullable = true)\n |-- lon: string (nullable = true)\n |-- lat: string (nullable = true)\n\n"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": 5, 
            "metadata": {
                "collapsed": false
            }, 
            "source": "sc", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "<pyspark.context.SparkContext at 0x7fde33abfed0>"
                    }, 
                    "execution_count": 5, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": 6, 
            "metadata": {
                "collapsed": false
            }, 
            "source": "df_data_1.show()", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+---+-------------------+--------------------+--------------------+--------------------+------------------+---------------+----+--------------------+------------+----------+\n|_c0|        CASE_STATUS|       EMPLOYER_NAME|            SOC_NAME|           JOB_TITLE|FULL_TIME_POSITION|PREVAILING_WAGE|YEAR|            WORKSITE|         lon|       lat|\n+---+-------------------+--------------------+--------------------+--------------------+------------------+---------------+----+--------------------+------------+----------+\n|  1|CERTIFIED-WITHDRAWN|UNIVERSITY OF MIC...|BIOCHEMISTS AND B...|POSTDOCTORAL RESE...|                 N|          36067|2016| ANN ARBOR, MICHIGAN| -83.7430378|42.2808256|\n|  2|CERTIFIED-WITHDRAWN|GOODMAN NETWORKS,...|    CHIEF EXECUTIVES|CHIEF OPERATING O...|                 Y|         242674|2016|        PLANO, TEXAS| -96.6988856|33.0198431|\n|  3|CERTIFIED-WITHDRAWN|PORTS AMERICA GRO...|    CHIEF EXECUTIVES|CHIEF PROCESS OFF...|                 Y|         193066|2016|JERSEY CITY, NEW ...| -74.0776417|40.7281575|\n|  4|CERTIFIED-WITHDRAWN|GATES CORPORATION...|    CHIEF EXECUTIVES|REGIONAL PRESIDEN...|                 Y|         220314|2016|    DENVER, COLORADO| -104.990251|39.7392358|\n|  5|          WITHDRAWN|PEABODY INVESTMEN...|    CHIEF EXECUTIVES|PRESIDENT MONGOLI...|                 Y|       157518.4|2016| ST. LOUIS, MISSOURI| -90.1994042|38.6270025|\n|  6|CERTIFIED-WITHDRAWN|BURGER KING CORPO...|    CHIEF EXECUTIVES|EXECUTIVE V P, GL...|                 Y|         225000|2016|      MIAMI, FLORIDA| -80.1917902|25.7616798|\n|  7|CERTIFIED-WITHDRAWN|BT AND MK ENERGY ...|    CHIEF EXECUTIVES|CHIEF OPERATING O...|                 Y|          91021|2016|      HOUSTON, TEXAS| -95.3698028|29.7604267|\n|  8|CERTIFIED-WITHDRAWN|GLOBO MOBILE TECH...|    CHIEF EXECUTIVES|CHIEF OPERATIONS ...|                 Y|         150000|2016|SAN JOSE, CALIFORNIA|-121.8863286|37.3382082|\n|  9|CERTIFIED-WITHDRAWN|  ESI COMPANIES INC.|    CHIEF EXECUTIVES|           PRESIDENT|                 Y|         127546|2016|      MEMPHIS, TEXAS|          NA|        NA|\n| 10|          WITHDRAWN|LESSARD INTERNATI...|    CHIEF EXECUTIVES|           PRESIDENT|                 Y|         154648|2016|    VIENNA, VIRGINIA| -77.2652604|38.9012225|\n| 11|CERTIFIED-WITHDRAWN|  H.J. HEINZ COMPANY|    CHIEF EXECUTIVES|CHIEF INFORMATION...|                 Y|         182978|2016|PITTSBURGH, PENNS...| -79.9958864|40.4406248|\n| 12|CERTIFIED-WITHDRAWN|DOW CORNING CORPO...|    CHIEF EXECUTIVES|VICE PRESIDENT AN...|                 Y|         163717|2016|   MIDLAND, MICHIGAN| -84.2472116|43.6155825|\n| 13|CERTIFIED-WITHDRAWN|    ACUSHNET COMPANY|    CHIEF EXECUTIVES|   TREASURER AND COO|                 Y|       203860.8|2016|FAIRHAVEN, MASSAC...|          NA|        NA|\n| 14|CERTIFIED-WITHDRAWN|       BIOCAIR, INC.|    CHIEF EXECUTIVES|CHIEF COMMERCIAL ...|                 Y|         252637|2016|      MIAMI, FLORIDA| -80.1917902|25.7616798|\n| 15|CERTIFIED-WITHDRAWN|NEWMONT MINING CO...|    CHIEF EXECUTIVES|        BOARD MEMBER|                 Y|         105914|2016|GREENWOOD VILLAGE...|-104.9508141|39.6172101|\n| 16|CERTIFIED-WITHDRAWN|        VRICON, INC.|    CHIEF EXECUTIVES|CHIEF FINANCIAL O...|                 Y|         153046|2016|  STERLING, VIRGINIA| -77.4291298|39.0066993|\n| 17|CERTIFIED-WITHDRAWN|CARDIAC SCIENCE C...|  FINANCIAL MANAGERS|VICE PRESIDENT OF...|                 Y|          90834|2016| WAUKESHA, WISCONSIN| -88.2314813|43.0116784|\n| 18|CERTIFIED-WITHDRAWN|WESTFIELD CORPORA...|    CHIEF EXECUTIVES|GENERAL MANAGER, ...|                 Y|         164050|2016|LOS ANGELES, CALI...|-118.2436849|34.0522342|\n| 19|          CERTIFIED|      QUICKLOGIX LLC|    CHIEF EXECUTIVES|                 CEO|                 Y|         187200|2016|SANTA CLARA, CALI...|-121.9552356|37.3541079|\n| 20|          CERTIFIED|MCCHRYSTAL GROUP,...|    CHIEF EXECUTIVES|PRESIDENT, NORTHE...|                 Y|         241842|2016|ALEXANDRIA, VIRGINIA| -77.0469214|38.8048355|\n+---+-------------------+--------------------+--------------------+--------------------+------------------+---------------+----+--------------------+------------+----------+\nonly showing top 20 rows\n\n"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": 7, 
            "metadata": {
                "collapsed": false
            }, 
            "source": "df_data_1.groupBy('SOC_NAME').count().show()", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+--------------------+-----+\n|            SOC_NAME|count|\n+--------------------+-----+\n|Software Develope...|  920|\n|Food Service Mana...| 1242|\n|Computer Hardware...|   17|\n|Human Resources A...|   94|\n|COMPUTER & INFORM...|    6|\n|FOOD SERVICE MANA...|  370|\n|PROPERTY, REAL ES...|    2|\n|AGENTS AND BUSINE...|   96|\n|     COST ESTIMATORS| 1451|\n|SOFTWARE QUALITY ...|    3|\n|CHEMICAL TECHNICIANS|  173|\n|CAREER/TECHNICAL ...|    3|\n|INSTRUCTIONAL COO...| 1145|\n|MUSIC DIRECTORS A...|  138|\n|INDUSTRIAL DESIGNERS|    1|\n|NETWORK SECURITY ...|    1|\n|AIRCRAFT CARGO HA...|    1|\n|Vocational Educat...|    3|\n|Property, Real Es...|  283|\n|  Recreation Workers|  342|\n+--------------------+-----+\nonly showing top 20 rows\n\n"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": 8, 
            "metadata": {
                "collapsed": false
            }, 
            "source": "test = df_data_1.rdd.map(lambda s: str(s).lower())", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "Here, we lowercase everything in the dataframe"
        }, 
        {
            "cell_type": "code", 
            "execution_count": 9, 
            "metadata": {
                "collapsed": false
            }, 
            "source": "test.take(5)", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "[\"row(_c0=1, case_status=u'certified-withdrawn', employer_name=u'university of michigan', soc_name=u'biochemists and biophysicists', job_title=u'postdoctoral research fellow', full_time_position=u'n', prevailing_wage=u'36067', year=u'2016', worksite=u'ann arbor, michigan', lon=u'-83.7430378', lat=u'42.2808256')\",\n \"row(_c0=2, case_status=u'certified-withdrawn', employer_name=u'goodman networks, inc.', soc_name=u'chief executives', job_title=u'chief operating officer', full_time_position=u'y', prevailing_wage=u'242674', year=u'2016', worksite=u'plano, texas', lon=u'-96.6988856', lat=u'33.0198431')\",\n \"row(_c0=3, case_status=u'certified-withdrawn', employer_name=u'ports america group, inc.', soc_name=u'chief executives', job_title=u'chief process officer', full_time_position=u'y', prevailing_wage=u'193066', year=u'2016', worksite=u'jersey city, new jersey', lon=u'-74.0776417', lat=u'40.7281575')\",\n \"row(_c0=4, case_status=u'certified-withdrawn', employer_name=u'gates corporation, a wholly-owned subsidiary of tomkins plc', soc_name=u'chief executives', job_title=u'regional presiden, americas', full_time_position=u'y', prevailing_wage=u'220314', year=u'2016', worksite=u'denver, colorado', lon=u'-104.990251', lat=u'39.7392358')\",\n \"row(_c0=5, case_status=u'withdrawn', employer_name=u'peabody investments corp.', soc_name=u'chief executives', job_title=u'president mongolia and india', full_time_position=u'y', prevailing_wage=u'157518.4', year=u'2016', worksite=u'st. louis, missouri', lon=u'-90.1994042', lat=u'38.6270025')\"]"
                    }, 
                    "execution_count": 9, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": 16, 
            "metadata": {
                "collapsed": false
            }, 
            "source": "test.groupBy('soc_name')", 
            "outputs": [
                {
                    "output_type": "error", 
                    "traceback": [
                        "\u001b[1;31m\u001b[0m", 
                        "\u001b[1;31mPy4JJavaError\u001b[0mTraceback (most recent call last)", 
                        "\u001b[1;32m<ipython-input-16-bb0317f3b541>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'soc_name'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m", 
                        "\u001b[1;32m/usr/local/src/spark20master/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1310\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1312\u001b[0m             \u001b[0mitems\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark20master/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[1;31m# SparkContext#runJob.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m         \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark20master/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark20master/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark20master/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m                 raise Py4JError(\n", 
                        "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 13.0 failed 10 times, most recent failure: Lost task 0.9 in stage 13.0 (TID 66, yp-spark-dal09-env5-0042): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/src/spark20master/spark-2.0.2-bin-2.7.3/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/usr/local/src/spark20master/spark-2.0.2-bin-2.7.3/python/lib/pyspark.zip/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/src/spark20master/spark/python/pyspark/rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/src/spark20master/spark/python/pyspark/rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/src/spark20master/spark/python/pyspark/rdd.py\", line 317, in func\n    return f(iterator)\n  File \"/usr/local/src/spark20master/spark/python/pyspark/rdd.py\", line 1876, in combine\n    merger.mergeValues(iterator)\n  File \"/usr/local/src/spark20master/spark-2.0.2-bin-2.7.3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 236, in mergeValues\n    for k, v in iterator:\n  File \"/usr/local/src/spark20master/spark/python/pyspark/rdd.py\", line 701, in <lambda>\n    return self.map(lambda x: (f(x), x)).groupByKey(numPartitions, partitionFunc)\nTypeError: 'str' object is not callable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:390)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1153)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.lang.Thread.run(Thread.java:785)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1461)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1449)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1448)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1448)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:812)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:812)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:812)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1674)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1629)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat java.lang.Thread.getStackTrace(Thread.java:1117)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:633)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1887)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1900)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1913)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:507)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:785)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/src/spark20master/spark-2.0.2-bin-2.7.3/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/usr/local/src/spark20master/spark-2.0.2-bin-2.7.3/python/lib/pyspark.zip/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/src/spark20master/spark/python/pyspark/rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/src/spark20master/spark/python/pyspark/rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/src/spark20master/spark/python/pyspark/rdd.py\", line 317, in func\n    return f(iterator)\n  File \"/usr/local/src/spark20master/spark/python/pyspark/rdd.py\", line 1876, in combine\n    merger.mergeValues(iterator)\n  File \"/usr/local/src/spark20master/spark-2.0.2-bin-2.7.3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 236, in mergeValues\n    for k, v in iterator:\n  File \"/usr/local/src/spark20master/spark/python/pyspark/rdd.py\", line 701, in <lambda>\n    return self.map(lambda x: (f(x), x)).groupByKey(numPartitions, partitionFunc)\nTypeError: 'str' object is not callable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:390)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1153)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
                    ], 
                    "ename": "Py4JJavaError", 
                    "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 13.0 failed 10 times, most recent failure: Lost task 0.9 in stage 13.0 (TID 66, yp-spark-dal09-env5-0042): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/src/spark20master/spark-2.0.2-bin-2.7.3/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/usr/local/src/spark20master/spark-2.0.2-bin-2.7.3/python/lib/pyspark.zip/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/src/spark20master/spark/python/pyspark/rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/src/spark20master/spark/python/pyspark/rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/src/spark20master/spark/python/pyspark/rdd.py\", line 317, in func\n    return f(iterator)\n  File \"/usr/local/src/spark20master/spark/python/pyspark/rdd.py\", line 1876, in combine\n    merger.mergeValues(iterator)\n  File \"/usr/local/src/spark20master/spark-2.0.2-bin-2.7.3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 236, in mergeValues\n    for k, v in iterator:\n  File \"/usr/local/src/spark20master/spark/python/pyspark/rdd.py\", line 701, in <lambda>\n    return self.map(lambda x: (f(x), x)).groupByKey(numPartitions, partitionFunc)\nTypeError: 'str' object is not callable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:390)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1153)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.lang.Thread.run(Thread.java:785)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1461)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1449)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1448)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1448)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:812)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:812)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:812)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1674)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1629)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat java.lang.Thread.getStackTrace(Thread.java:1117)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:633)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1887)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1900)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1913)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:507)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:785)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/src/spark20master/spark-2.0.2-bin-2.7.3/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/usr/local/src/spark20master/spark-2.0.2-bin-2.7.3/python/lib/pyspark.zip/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/src/spark20master/spark/python/pyspark/rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/src/spark20master/spark/python/pyspark/rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/src/spark20master/spark/python/pyspark/rdd.py\", line 317, in func\n    return f(iterator)\n  File \"/usr/local/src/spark20master/spark/python/pyspark/rdd.py\", line 1876, in combine\n    merger.mergeValues(iterator)\n  File \"/usr/local/src/spark20master/spark-2.0.2-bin-2.7.3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 236, in mergeValues\n    for k, v in iterator:\n  File \"/usr/local/src/spark20master/spark/python/pyspark/rdd.py\", line 701, in <lambda>\n    return self.map(lambda x: (f(x), x)).groupByKey(numPartitions, partitionFunc)\nTypeError: 'str' object is not callable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:390)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1153)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": 11, 
            "metadata": {
                "collapsed": false
            }, 
            "source": "rdd_data_1=df_data_1.rdd", 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "execution_count": 12, 
            "metadata": {
                "collapsed": false
            }, 
            "source": "newdata=rdd_data_1.map(lambda x: x.lower())", 
            "outputs": []
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 2 with Spark 2.0", 
            "language": "python", 
            "name": "python2-spark20"
        }, 
        "language_info": {
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }, 
            "version": "2.7.11", 
            "name": "python", 
            "file_extension": ".py", 
            "nbconvert_exporter": "python", 
            "mimetype": "text/x-python", 
            "pygments_lexer": "ipython2"
        }
    }, 
    "nbformat_minor": 0
}